# World Model Evolution Roadmap

**Goal**: Evolve from current working model to next-gen architecture (2026-competitive)

**Principle**: Each phase is self-contained, testable, and preserves previous functionality.

---

## Current State (v1.0) âœ…

```
Single Frame â†’ VQ-VAE â†’ Tokens â†’ Transformer â†’ Tokens â†’ VQ-VAE â†’ Next Frame
                                      â†‘
                                   Action
```

| Component | Status |
|-----------|--------|
| VQ-VAE (128 codes) | Working |
| Transformer (6L, 256d) | Working |
| Single frame context | Working |
| 64Ã—64 resolution | Working |
| 4 discrete actions | Working |

---

## Phase 1: Temporal Conditioning ğŸ¯

**Priority**: HIGH  
**Estimated time**: 1-2 weeks  
**Goal**: Model sees past N frames, not just current frame

### 1.1 Architecture Change

```
CURRENT (v1.0)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[frame_t tokens] + [action] â†’ Transformer â†’ [frame_t+1 tokens]

NEW (v1.1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[frame_t-3] [frame_t-2] [frame_t-1] [frame_t] + [action] â†’ Transformer â†’ [frame_t+1]
     â†“           â†“           â†“          â†“
   pos=0       pos=1       pos=2      pos=3     (temporal position embedding)
```

### 1.2 Implementation Tasks

```
â–¡ 1.1.1  Create TemporalVisualWorldModel class (new file, don't modify original)
â–¡ 1.1.2  Add frame history buffer in data collection
â–¡ 1.1.3  Modify dataset to return (frame_history, action, next_frame) tuples
â–¡ 1.1.4  Add temporal position embeddings (separate from spatial)
â–¡ 1.1.5  Train on same game data with history context
â–¡ 1.1.6  Compare metrics: v1.0 vs v1.1 accuracy
â–¡ 1.1.7  Side-by-side visual comparison
```

### 1.3 Key Code Changes

```python
# New temporal embedding
self.temporal_embed = nn.Embedding(max_history + 1, d_model)  # +1 for prediction

# Forward with history
def forward(self, frame_history, action, target=None):
    """
    frame_history: (batch, n_frames, n_tokens)  # e.g., (B, 4, 64)
    action: (batch,)
    """
    B, T, N = frame_history.shape
    
    # Embed all frames
    all_tokens = []
    for t in range(T):
        frame_emb = self.token_embed(frame_history[:, t, :])
        frame_emb = frame_emb + self.spatial_pos(...)
        frame_emb = frame_emb + self.temporal_embed(t)  # NEW
        all_tokens.append(frame_emb)
    
    # Rest of forward...
```

### 1.4 Expected Improvements

| Metric | v1.0 | v1.1 (expected) |
|--------|------|-----------------|
| Token accuracy | 42% | 55-65% |
| Motion blur handling | Poor | Good |
| Object permanence | None | Some |
| Temporal consistency | Poor | Better |

---

## Phase 2: Causal (Autoregressive) Token Prediction

**Priority**: MEDIUM  
**Estimated time**: 1 week  
**Goal**: Predict tokens one-by-one for better spatial coherence

### 2.1 Architecture Change

```
CURRENT: Parallel prediction (all 64 tokens at once)
NEW: Autoregressive (predict token 0, then 1, then 2, ...)

Can use BOTH modes:
- Autoregressive for quality (offline generation)
- Parallel for speed (real-time play)
```

### 2.2 Implementation Tasks

```
â–¡ 2.2.1  Add causal attention mask option
â–¡ 2.2.2  Implement autoregressive sampling loop
â–¡ 2.2.3  Add KV-cache for efficient autoregressive inference
â–¡ 2.2.4  Compare quality: parallel vs autoregressive
â–¡ 2.2.5  Benchmark speed tradeoff
```

---

## Phase 3: Higher Resolution

**Priority**: MEDIUM  
**Estimated time**: 2 weeks  
**Goal**: 64Ã—64 â†’ 128Ã—128 or 256Ã—256

### 3.1 Challenges

```
Resolution    Tokens    Transformer cost
64Ã—64         8Ã—8=64    O(64Â²) = 4K
128Ã—128       16Ã—16=256 O(256Â²) = 65K   â† 16Ã— more expensive
256Ã—256       32Ã—32=1024 O(1024Â²) = 1M  â† 256Ã— more expensive
```

### 3.2 Solutions

```
â–¡ 3.2.1  Hierarchical VQ-VAE (multi-scale tokens)
â–¡ 3.2.2  Patch-based processing (Oasis-style tiling)
â–¡ 3.2.3  Sparse attention patterns
â–¡ 3.2.4  Latent downsampling before transformer
```

---

## Phase 4: Diffusion Decoder

**Priority**: MEDIUM  
**Estimated time**: 2-3 weeks  
**Goal**: Replace VQ-VAE decoder with conditional diffusion

### 4.1 Architecture

```
Current:  Tokens â†’ VQ-VAE Decoder â†’ Frame
New:      Tokens â†’ Condition â†’ Diffusion U-Net â†’ Frame
                                    â†‘
                              4-8 denoising steps
```

### 4.2 Implementation Tasks

```
â–¡ 4.2.1  Create small U-Net conditioned on token embeddings
â–¡ 4.2.2  Implement DDPM/DDIM training
â–¡ 4.2.3  Train decoder on (tokens, frame) pairs from existing data
â–¡ 4.2.4  Tune step count for speed/quality tradeoff
â–¡ 4.2.5  A/B test visual quality
```

---

## Phase 5: Latent Diffusion Dynamics

**Priority**: LOW (major architecture shift)  
**Estimated time**: 4+ weeks  
**Goal**: Replace transformer token prediction with DiT in latent space

### 5.1 Architecture (Oasis-style)

```
Frame â†’ VAE Encoder â†’ Latent (32Ã—32Ã—4)
                           â†“
              Diffusion Transformer (DiT)
              [latent_noisy, action, timestep] â†’ noise_pred
                           â†“
                    Denoise (4-8 steps)
                           â†“
               VAE Decoder â†’ Next Frame
```

### 5.2 This is a bigger rewrite - defer until Phases 1-4 complete

---

## Phase 6: Multi-Game Training

**Priority**: LOW  
**Estimated time**: Ongoing  
**Goal**: Single model that works across multiple games

### 6.1 Requirements

```
â–¡ 6.1.1  Game-agnostic action representation
â–¡ 6.1.2  Multiple game data loaders
â–¡ 6.1.3  Game conditioning token/embedding
â–¡ 6.1.4  Larger model capacity
```

---

## Immediate Next Steps

### Week 1: Setup

```
â–¡ Create v2/ directory for new development
â–¡ Copy working v1 files as starting point
â–¡ Set up comparison infrastructure (v1 vs v2 metrics)
â–¡ Create temporal dataset loader
```

### Week 2: Temporal v1.1

```
â–¡ Implement TemporalVisualWorldModel
â–¡ Train with 4-frame history
â–¡ Evaluate and compare
â–¡ Document findings
```

---

## File Structure (Proposed)

```
world_model/
â”œâ”€â”€ v1/                          # Current working model (frozen)
â”‚   â”œâ”€â”€ vqvae.py
â”‚   â”œâ”€â”€ visual_world_model.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ v2/                          # Next-gen development
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ vqvae.py            # Copy from v1, may modify later
â”‚   â”‚   â”œâ”€â”€ temporal_world_model.py   # Phase 1
â”‚   â”‚   â”œâ”€â”€ causal_world_model.py     # Phase 2
â”‚   â”‚   â””â”€â”€ diffusion_decoder.py      # Phase 4
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ temporal_dataset.py  # Frame history support
â”‚   â”‚   â””â”€â”€ game_env.py          # Upgraded environment
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ train_temporal.py
â”‚   â”‚   â””â”€â”€ train_diffusion.py
â”‚   â””â”€â”€ eval/
â”‚       â”œâ”€â”€ compare_versions.py
â”‚       â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ v1/                      # v1 weights (don't touch)
â”‚   â””â”€â”€ v2/                      # v2 experiments
â”‚
â””â”€â”€ ROADMAP.md                   # This file
```

---

## Success Metrics

| Metric | v1.0 | v2.0 Target |
|--------|------|-------------|
| Token Accuracy | 42% | 75%+ |
| Visual Quality (subjective) | Good | Excellent |
| Temporal Consistency | Poor | Good |
| Resolution | 64Ã—64 | 128Ã—128+ |
| Real-time FPS | 30+ | 20+ |
| Model Size | 11M | 50-100M |

---

## Risk Mitigation

1. **Never modify v1/** - Only add new code in v2/
2. **Version checkpoints** - v1/model.pt, v2/model.pt, etc.
3. **Comparison scripts** - Always measure before/after
4. **Incremental commits** - One feature per PR/commit
5. **Regression tests** - Ensure v1 still works after each change

---

## Ready to Start?

Phase 1.1.1: Create the temporal world model architecture.

Switch to Agent mode and say "start phase 1" to begin.




