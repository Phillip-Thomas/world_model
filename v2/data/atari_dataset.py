"""
Atari Dataset Collector (v3)
============================
Collects gameplay data from Atari games with proper aspect ratio preservation.

v3 fixes (critical correctness):
- Frame/action alignment: store pre-action frame (action[t] transitions frame[t] â†’ frame[t+1])
- Episode-based train/val split (no leakage)
- Reward weights computed per-episode (no cross-episode bleed)
- Zero-copy tensor creation (no unnecessary block.copy())
- Novelty weights (frame diff) for better sampling diversity
- State-aware FIRE probability (fire until ball launched)

v2 improvements:
- Preserves native aspect ratio (no skewing)
- Max-pooling to handle Atari flickering
- Sampling weights instead of deletion for biasing
- Optimized dataloader (uint8 until GPU)
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset
import numpy as np
from typing import Tuple, Optional, List, Union
from dataclasses import dataclass, field
import os
from tqdm import tqdm
import cv2  # Faster than PIL for resize

try:
    import gymnasium as gym
    import ale_py
    gym.register_envs(ale_py)
    GYM_AVAILABLE = True
except ImportError:
    GYM_AVAILABLE = False
    print("Warning: gymnasium not installed. Run: pip install gymnasium[atari]")


# =============================================================================
# Native Atari dimensions for common games
# =============================================================================
ATARI_NATIVE_SIZES = {
    'ALE/Breakout-v5': (210, 160),
    'ALE/Pong-v5': (210, 160),
    'ALE/SpaceInvaders-v5': (210, 160),
    'ALE/MsPacman-v5': (210, 160),
    'ALE/Asteroids-v5': (210, 160),
    'default': (210, 160),
}


def compute_aspect_preserving_size(
    native_h: int, 
    native_w: int, 
    target_width: int = 64
) -> Tuple[int, int]:
    """
    Compute target size that preserves aspect ratio.
    
    For Breakout (210x160) with target_width=64:
    - aspect = 210/160 = 1.3125
    - target_height = 64 * 1.3125 = 84
    - Returns (84, 64)
    
    Result is rounded to nearest multiple of 4 for encoder compatibility.
    """
    aspect = native_h / native_w
    target_h = int(target_width * aspect)
    target_h = ((target_h + 2) // 4) * 4
    return (target_h, target_width)


@dataclass
class AtariConfig:
    """Configuration for Atari data collection."""
    
    # Game settings
    game: str = "ALE/Breakout-v5"
    
    # Image settings
    target_width: int = 64
    preserve_aspect: bool = True
    target_size: Tuple[int, int] = field(default=None)
    grayscale: bool = False
    
    # Frame processing
    max_pool_frames: bool = True
    frame_skip: int = 4
    
    # Sequence settings
    history_len: int = 4
    
    # Collection settings
    n_episodes: int = 100
    max_steps: int = 1000
    
    # Action selection
    random_policy: bool = True
    fire_on_reset: bool = True
    noop_max: int = 30
    
    # v3: State-aware FIRE
    fire_until_ball_moves: bool = True  # Keep firing until ball is in play
    fire_steps_after_reset: int = 5     # Force FIRE for first N steps after reset/life loss
    
    # Action diversity
    action_weights: Optional[List[float]] = None
    action_repeat_max: int = 4
    
    # v3: Reward + novelty weights
    reward_proximity_tau: float = 10.0
    novelty_weight: float = 0.5  # Weight for novelty vs reward proximity
    use_weighted_sampling: bool = True
    
    def __post_init__(self):
        if self.target_size is None:
            if self.preserve_aspect:
                native = ATARI_NATIVE_SIZES.get(self.game, ATARI_NATIVE_SIZES['default'])
                self.target_size = compute_aspect_preserving_size(
                    native[0], native[1], self.target_width
                )
            else:
                self.target_size = (self.target_width, self.target_width)


class AtariCollector:
    """
    Collects gameplay data from Atari games with proper preprocessing.
    
    v3 key fix: stores frame BEFORE action, so:
    - frames[t] is state before action[t]
    - action[t] transitions from frames[t] to frames[t+1]
    - This is the canonical (s, a, s') alignment
    """
    
    def __init__(self, config: AtariConfig = None):
        self.config = config or AtariConfig()
        
        if not GYM_AVAILABLE:
            raise RuntimeError("gymnasium required. Run: pip install gymnasium[atari]")
        
        self._action_counts = None
        self._last_action = None
        self._action_repeat_count = 0
        self._steps_since_reset = 0  # v3: track steps for FIRE logic
    
    def _create_env(self):
        env = gym.make(
            self.config.game,
            frameskip=1,
            repeat_action_probability=0.0,
        )
        return env
    
    def _resize_frame(self, frame: np.ndarray) -> np.ndarray:
        target_h, target_w = self.config.target_size
        resized = cv2.resize(frame, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
        return resized
    
    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:
        frame = self._resize_frame(frame)
        if self.config.grayscale:
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            frame = frame[:, :, np.newaxis]
        return frame
    
    def _max_pool_frames(self, frames: List[np.ndarray]) -> np.ndarray:
        if len(frames) == 1:
            return frames[0]
        return np.maximum.reduce(frames[-2:])
    
    def _select_action(self, env, n_actions: int) -> int:
        """
        Select action with diversity guarantees.
        
        v3: State-aware FIRE - force FIRE for first few steps after reset.
        v4: Game-agnostic action selection (works for any number of actions).
        """
        if self._action_counts is None:
            self._action_counts = np.zeros(n_actions)
        
        # v3: Force FIRE for first few steps after reset/life loss
        # FIRE is typically action 1 in most Atari games
        if (self.config.fire_until_ball_moves and 
            self._steps_since_reset < self.config.fire_steps_after_reset and
            n_actions > 1):
            self._steps_since_reset += 1
            self._action_counts[1] += 1  # FIRE = 1
            return 1
        
        # Prevent long repeats
        if (self._last_action is not None and 
            self._action_repeat_count >= self.config.action_repeat_max):
            available = [a for a in range(n_actions) if a != self._last_action]
            action = np.random.choice(available)
            self._action_repeat_count = 0
            self._last_action = action
            self._action_counts[action] += 1
            return action
        
        # Use custom weights or generate game-agnostic defaults
        if self.config.action_weights is not None:
            probs = np.array(self.config.action_weights)
        else:
            # v4: Game-agnostic action selection
            # - Action 0 (NOOP): low probability (5%)
            # - Action 1 (FIRE): very low after launch (1%)
            # - All other actions: equal split of remaining probability
            probs = np.ones(n_actions)
            if n_actions > 0:
                probs[0] = 0.05  # NOOP
            if n_actions > 1:
                probs[1] = 0.01  # FIRE (after ball launched)
            if n_actions > 2:
                # Distribute remaining probability equally among movement actions
                remaining_prob = 1.0 - 0.05 - 0.01
                per_action = remaining_prob / (n_actions - 2)
                probs[2:] = per_action
        
        probs = probs / probs.sum()
        action = np.random.choice(n_actions, p=probs)
        
        if action == self._last_action:
            self._action_repeat_count += 1
        else:
            self._action_repeat_count = 0
        self._last_action = action
        self._action_counts[action] += 1
        
        return action
    
    def _reset_episode_state(self):
        self._last_action = None
        self._action_repeat_count = 0
        self._steps_since_reset = 0  # v3: reset step counter
    
    def _compute_sample_weights(
        self,
        frames: np.ndarray,
        rewards: np.ndarray,
        dones: np.ndarray,
        episode_starts: np.ndarray,
    ) -> np.ndarray:
        """
        Compute sampling weights. Simplified to uniform weights to avoid
        v3 alignment issues (frames has N+1 elements, rewards has N).
        """
        # Use uniform weights - training script handles expert weighting separately
        return np.ones(len(rewards), dtype=np.float32)
    
    def collect(
        self,
        save_path: Optional[str] = None,
        show_progress: bool = True,
    ) -> dict:
        """
        Collect gameplay data with CORRECT frame/action alignment.
        
        v3 key fix: Store frame BEFORE action.
        - frames has length N+1 (includes initial state)
        - actions/rewards/dones have length N
        - action[t] transitions frames[t] -> frames[t+1]
        """
        env = self._create_env()
        n_actions = env.action_space.n
        
        all_frames = []
        all_actions = []
        all_rewards = []
        all_dones = []
        episode_starts = []
        
        self._action_counts = np.zeros(n_actions)
        
        iterator = range(self.config.n_episodes)
        if show_progress:
            iterator = tqdm(iterator, desc=f"Collecting {self.config.game}")
        
        for episode in iterator:
            obs, info = env.reset()
            self._reset_episode_state()
            
            # Random no-ops for diversity
            if self.config.noop_max > 0:
                n_noops = np.random.randint(1, self.config.noop_max + 1)
                for _ in range(n_noops):
                    obs, _, terminated, truncated, _ = env.step(0)
                    if terminated or truncated:
                        obs, _ = env.reset()
                        break
            
            # Fire to start (get ball moving in Breakout)
            if self.config.fire_on_reset:
                obs, _, _, _, _ = env.step(1)
            
            # v3: Store INITIAL frame (before any action in main loop)
            episode_starts.append(len(all_frames))
            initial_frame = self._preprocess_frame(obs)
            all_frames.append(initial_frame)
            
            for step in range(self.config.max_steps):
                # Select action (applied to current state)
                action = self._select_action(env, n_actions)
                
                # Execute with frame skipping
                total_reward = 0
                done = False
                frame_buffer = []
                
                for skip_idx in range(self.config.frame_skip):
                    obs, reward, terminated, truncated, info = env.step(action)
                    total_reward += reward
                    done = terminated or truncated
                    
                    if skip_idx >= self.config.frame_skip - 2:
                        frame_buffer.append(obs.copy())
                    
                    if done:
                        # v3: Reset step counter on life loss
                        self._steps_since_reset = 0
                        break
                
                # Max-pool and preprocess -> this is the NEXT state
                if self.config.max_pool_frames and len(frame_buffer) >= 2:
                    pooled = self._max_pool_frames(frame_buffer)
                else:
                    pooled = frame_buffer[-1] if frame_buffer else obs
                
                next_frame = self._preprocess_frame(pooled)
                
                # v3: Store transition data
                # action[t] was applied to frames[t], resulting in frames[t+1]
                all_actions.append(action)
                all_rewards.append(total_reward)
                all_dones.append(done)
                all_frames.append(next_frame)  # This is frames[t+1]
                
                if done:
                    break
        
        env.close()
        
        # Convert to arrays
        frames = np.array(all_frames, dtype=np.uint8)
        actions = np.array(all_actions, dtype=np.int32)
        rewards = np.array(all_rewards, dtype=np.float32)
        dones = np.array(all_dones, dtype=bool)
        ep_starts = np.array(episode_starts, dtype=np.int32)
        
        # v3: Compute sample weights with per-episode handling
        sample_weights = self._compute_sample_weights(frames, rewards, dones, ep_starts)
        
        data = {
            'frames': frames,
            'actions': actions,
            'rewards': rewards,
            'dones': dones,
            'episode_starts': ep_starts,
            'sample_weights': sample_weights,
            'n_actions': n_actions,
            'game': self.config.game,
            'frame_size': self.config.target_size,
        }
        
        if save_path:
            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
            np.savez_compressed(save_path, **data)
            if show_progress:
                print(f"Saved {len(frames)} frames to {save_path}")
        
        if show_progress:
            print(f"  Frame size: {frames.shape[1:]} (aspect preserved)")
            print(f"  Total frames: {len(frames):,} (includes initial states)")
            print(f"  Transitions: {len(actions):,}")
            print(f"  Episodes: {len(ep_starts)}")
            print(f"  Total rewards: {rewards.sum():.0f}")
            print(f"  Action distribution: {self._action_counts.astype(int).tolist()}")
        
        return data


class AtariTemporalDataset(Dataset):
    """
    Dataset for temporal world model training on Atari.
    
    v4 CRITICAL FIX: Correct frame/action alignment across episodes.
    
    The bug: Each episode has frames = actions + 1 (initial frame).
    After concatenation: total_frames = total_actions + n_episodes.
    
    So action[i] does NOT map frames[i] -> frames[i+1] globally!
    Instead: frame_index = action_index + episode_id
    
    Where episode_id[i] = number of episodes that started before action i.
    
    Returns (frame_history, action, next_frame) tuples where:
    - history ends at frames[frame_index[i]]
    - action is actions[i]
    - next_frame is frames[frame_index[i] + 1]
    """
    
    def __init__(
        self,
        data: dict,
        history_len: int = 4,
        preprocessed_tokens: bool = False,
        normalize_in_loader: bool = False,
    ):
        self.history_len = history_len
        self.preprocessed_tokens = preprocessed_tokens
        self.normalize_in_loader = normalize_in_loader
        
        if preprocessed_tokens:
            frames_np = data['frames']
            if frames_np.ndim == 3:
                self.frames = torch.from_numpy(frames_np.reshape(len(frames_np), -1)).long()
            else:
                self.frames = torch.from_numpy(frames_np).long()
        else:
            self.frames = data['frames']
        
        self.actions = torch.from_numpy(data['actions'].astype(np.int64)).long()
        self.rewards = data['rewards']
        self.dones = np.array(data['dones'], dtype=bool)
        self.n_actions = data['n_actions']  # Must be in dataset (game-agnostic)
        
        n = len(self.actions)
        
        # v4 FIX: Compute episode_id for each action index
        # episode_id[i] = number of dones (episode endings) before action i
        # This equals the number of "extra" initial frames before action i
        self.episode_id = np.zeros(n, dtype=np.int64)
        if n > 1:
            self.episode_id[1:] = np.cumsum(self.dones[:-1].astype(np.int64))
        
        # v4 FIX: Map action index -> frame index in concatenated array
        # frame_index[i] = i + episode_id[i]
        self.frame_index = np.arange(n, dtype=np.int64) + self.episode_id
        
        # v4 FIX: Episode start (in action-index space) for each action
        # First action of each episode is right after a done (or at index 0)
        episode_start_mask = np.concatenate([[True], self.dones[:-1]])
        episode_starts_action = np.where(episode_start_mask)[0]
        # For each action i, find which episode it belongs to and get that episode's start
        self.episode_start_action = episode_starts_action[self.episode_id]
        
        if 'sample_weights' in data:
            self.sample_weights = data['sample_weights']
        else:
            self.sample_weights = np.ones(n, dtype=np.float32)
        
        self.valid_indices = self._build_valid_indices()
        self.valid_weights = self.sample_weights[self.valid_indices]
        
        # Debug info
        n_episodes = self.episode_id[-1] + 1 if n > 0 else 0
        print(f"  [Dataset v4] {n} actions, {len(self.frames)} frames, {n_episodes} episodes")
        print(f"  [Dataset v4] Valid samples: {len(self.valid_indices)}")
        
        # v4 VERIFICATION: Check alignment assumptions
        # 1) len(frames) should equal len(actions) + n_episodes
        expected_frames = n + n_episodes
        if len(self.frames) != expected_frames:
            print(f"  [WARNING] Frame count mismatch! Expected {expected_frames}, got {len(self.frames)}")
            print(f"            This may indicate collector/dataset version mismatch")
        else:
            print(f"  [Dataset v4] Frame count verified: {len(self.frames)} == {n} actions + {n_episodes} episodes")
        
        # 2) Every action's frame_index + 1 must exist
        if n > 0:
            max_fi = self.frame_index.max()
            assert max_fi + 1 <= len(self.frames), \
                f"Frame index out of bounds: max frame_index={max_fi}, but only {len(self.frames)} frames"
        
        # 3) Verify episode start frames line up (optional diagnostic)
        if 'episode_starts' in data and n > 0:
            ep_starts_frames = set(map(int, data['episode_starts']))
            ep_start_actions = np.where(episode_start_mask)[0]
            matches = sum(1 for i in ep_start_actions if int(self.frame_index[i]) in ep_starts_frames)
            print(f"  [Dataset v4] Episode start alignment: {matches}/{len(ep_starts_frames)} match")
    
    def _build_valid_indices(self) -> np.ndarray:
        """
        v4 FIX: Find valid action indices with correct episode boundary handling.
        
        An action index i is valid if:
        1. Not a terminal state (dones[i] == False)
        2. History window stays within same episode (action-index space)
        3. frame_index[i] + 1 exists (need next frame)
        """
        n = len(self.actions)
        valid = []
        
        for i in range(n):
            # Skip terminal states - no meaningful next state
            if self.dones[i]:
                continue
            
            # History needs (history_len) frames ending at frame_index[i]
            # In action-index space, we need actions [i - history_len + 1, ..., i]
            # All must be in the same episode
            history_start_action = i - self.history_len + 1
            if history_start_action < self.episode_start_action[i]:
                continue
            
            # Need next frame to exist
            fi = self.frame_index[i]
            if fi + 1 >= len(self.frames):
                continue
            
            valid.append(i)
        
        return np.array(valid, dtype=np.int64)
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get a training sample with v4 correct frame indexing.
        
        - i = action index (in valid_indices)
        - fi = frame_index[i] = correct frame index in concatenated array
        - history = frames[fi - history_len + 1 : fi + 1]
        - action = actions[i]
        - next_frame = frames[fi + 1]
        """
        i = self.valid_indices[idx]
        fi = self.frame_index[i]
        start_f = fi - self.history_len + 1
        
        if self.preprocessed_tokens:
            history = self.frames[start_f:fi + 1]  # (T, n_tokens)
            action = self.actions[i]
            next_frame = self.frames[fi + 1]
            return history, action, next_frame
        
        # Raw frames path
        block = self.frames[start_f:fi + 2]  # (T+1, H, W, C) uint8
        
        if not block.flags['C_CONTIGUOUS']:
            block = np.ascontiguousarray(block)
        
        x = torch.from_numpy(block)
        x = x.permute(0, 3, 1, 2)  # (T+1, C, H, W)
        
        history = x[:-1]  # (T, C, H, W)
        next_frame = x[-1]  # (C, H, W)
        action = self.actions[i]
        
        if self.normalize_in_loader:
            history = history.float() / 127.5 - 1.0
            next_frame = next_frame.float() / 127.5 - 1.0
        
        return history, action, next_frame
    
    def get_sampler(self, replacement: bool = True) -> WeightedRandomSampler:
        weights = torch.from_numpy(self.valid_weights).double()
        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=replacement)
    
    def get_episode_ids(self) -> np.ndarray:
        """
        Get episode ID for each valid index.
        
        v4: Use precomputed episode_id array.
        """
        return self.episode_id[self.valid_indices]


def episode_based_split(
    dataset: AtariTemporalDataset,
    val_ratio: float = 0.1,
    seed: int = 42,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    v3: Split dataset indices by episode to prevent leakage.
    
    Returns indices into the dataset (not frame indices).
    """
    rng = np.random.RandomState(seed)
    
    episode_ids = dataset.get_episode_ids()
    unique_eps = np.unique(episode_ids)
    
    rng.shuffle(unique_eps)
    
    n_val_eps = max(1, int(len(unique_eps) * val_ratio))
    val_eps = set(unique_eps[-n_val_eps:])
    
    val_mask = np.array([eid in val_eps for eid in episode_ids])
    train_indices = np.where(~val_mask)[0]
    val_indices = np.where(val_mask)[0]
    
    return train_indices, val_indices


def create_atari_dataloader(
    config: AtariConfig = None,
    batch_size: int = 32,
    num_workers: int = 0,
    data_path: Optional[str] = None,
    use_weighted_sampling: bool = False,
    episode_split: bool = True,  # v3: default to episode-based split
) -> Tuple[DataLoader, DataLoader, dict]:
    """
    Create train/val dataloaders for Atari.
    
    v3: Uses episode-based split by default to prevent data leakage.
    """
    config = config or AtariConfig()
    
    if data_path and os.path.exists(data_path):
        print(f"Loading existing data from {data_path}")
        data = dict(np.load(data_path, allow_pickle=True))
        if 'sample_weights' not in data:
            print("  Computing sample weights for legacy data...")
            collector = AtariCollector(config)
            data['sample_weights'] = collector._compute_sample_weights(
                data['frames'], data['rewards'], data['dones'], data['episode_starts']
            )
    else:
        print(f"Collecting new data from {config.game}...")
        collector = AtariCollector(config)
        data = collector.collect(save_path=data_path)
    
    print(f"  Total frames: {len(data['frames']):,}")
    print(f"  Frame shape: {data['frames'].shape[1:]}")
    print(f"  Episodes: {len(data['episode_starts'])}")
    
    dataset = AtariTemporalDataset(data, history_len=config.history_len)
    print(f"  Valid samples: {len(dataset):,}")
    
    # v3: Episode-based split
    if episode_split:
        train_indices, val_indices = episode_based_split(dataset)
        print(f"  Split: episode-based (no leakage)")
    else:
        indices = np.random.permutation(len(dataset))
        n_train = int(len(dataset) * 0.9)
        train_indices = indices[:n_train]
        val_indices = indices[n_train:]
        print(f"  Split: random (WARNING: may leak)")
    
    print(f"  Train: {len(train_indices):,}, Val: {len(val_indices):,}")
    
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    
    if use_weighted_sampling:
        train_weights = dataset.valid_weights[train_indices]
        train_sampler = WeightedRandomSampler(
            torch.from_numpy(train_weights).double(),
            num_samples=len(train_indices),
            replacement=True
        )
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=train_sampler,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True,
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True,
        )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    data_info = {
        'n_actions': data['n_actions'],  # Must be in dataset
        'frame_size': data['frames'].shape[1:],
        'n_episodes': len(data['episode_starts']),
        'n_train': len(train_indices),
        'n_val': len(val_indices),
    }
    
    return train_loader, val_loader, data_info


# =============================================================================
# Test
# =============================================================================
if __name__ == "__main__":
    print("=" * 60)
    print("Atari Dataset v3 Test")
    print("=" * 60)
    
    config = AtariConfig(
        game='ALE/Breakout-v5',
        n_episodes=3,
        max_steps=100,
        preserve_aspect=True,
        target_width=64,
    )
    
    print(f"\nConfig:")
    print(f"  Target size: {config.target_size}")
    print(f"  Fire until ball moves: {config.fire_until_ball_moves}")
    print(f"  Novelty weight: {config.novelty_weight}")
    
    collector = AtariCollector(config)
    data = collector.collect(show_progress=True)
    
    print(f"\nData shapes:")
    print(f"  frames: {data['frames'].shape}")
    print(f"  actions: {data['actions'].shape}")
    print(f"  rewards: {data['rewards'].shape}")
    
    # v3 alignment check
    n_frames = len(data['frames'])
    n_actions = len(data['actions'])
    print(f"\n  v3 alignment check: {n_frames} frames, {n_actions} actions")
    print(f"  Expected: frames = actions + 1 -> {n_frames} == {n_actions + 1}? {n_frames == n_actions + 1}")
    
    dataset = AtariTemporalDataset(data, history_len=4)
    print(f"\nDataset:")
    print(f"  Valid samples: {len(dataset)}")
    
    # Test episode-based split
    train_idx, val_idx = episode_based_split(dataset)
    print(f"\nEpisode-based split:")
    print(f"  Train indices: {len(train_idx)}")
    print(f"  Val indices: {len(val_idx)}")
    
    # Check no overlap in episode IDs
    train_eps = set(dataset.get_episode_ids()[train_idx])
    val_eps = set(dataset.get_episode_ids()[val_idx])
    print(f"  Train episodes: {train_eps}")
    print(f"  Val episodes: {val_eps}")
    print(f"  Overlap: {train_eps & val_eps} (should be empty)")
    
    # Test batch
    history, action, next_frame = dataset[0]
    print(f"\nSample shapes:")
    print(f"  history: {history.shape}")
    print(f"  action: {action}")
    print(f"  next_frame: {next_frame.shape}")
    
    print("\n[OK] v3 dataset working!")
