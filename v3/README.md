# World Model v3

> **Discrete-token world model for Atari with continuous training.**

```
╔═══════════════════════════════════════════════════════════════════════════════════════════════════╗
║                                    WORLD MODEL ARCHITECTURE                                        ║
╠═══════════════════════════════════════════════════════════════════════════════════════════════════╣
║                                                                                                    ║
║      ┌─────────────────────────────────────────────────────────────────────────────────────────┐  ║
║      │                                   ATARI ENVIRONMENT                                      │  ║
║      │                                   (MsPacman, etc.)                                       │  ║
║      └─────────────────────────────┬───────────────────────────────────────▲────────────────────┘  ║
║                                    │ observation                           │ action               ║
║                                    ▼                                       │                      ║
║      ┌─────────────────────────────────────────────────────────────────────┼────────────────────┐  ║
║      │                                                                     │                    │  ║
║      │                            ┌───────────────────────┐                │                    │  ║
║      │                            │    RGB Frame 84×64    │                │                    │  ║
║      │                            └───────────┬───────────┘                │                    │  ║
║      │                                        │                            │                    │  ║
║      │      ┌─────────────────────────────────┴───────────────────────┐    │                    │  ║
║      │      │                                                         │    │                    │  ║
║      │      │                      VQ-VAE ENCODER                     │    │                    │  ║
║      │      │  ┌─────────────┐   ┌──────────────────┐   ┌──────────┐  │    │                    │  ║
║      │      │  │  Encoder    │   │ Vector Quantizer │   │ Discrete │  │    │                    │  ║
║      │      │  │  ConvNet    │──►│ 512 Codebook     │──►│ Tokens   │  │    │                    │  ║
║      │      │  │  4 layers   │   │ Entries          │   │ 21×16    │  │    │                    │  ║
║      │      │  └─────────────┘   └──────────────────┘   └──────────┘  │    │                    │  ║
║      │      │                                                         │    │                    │  ║
║      │      └─────────────────────────────┬───────────────────────────┘    │                    │  ║
║      │                                    │                                │                    │  ║
║      │                                    ▼                                │                    │  ║
║      │      ┌─────────────────────────────────────────────────────────────────────────────────┐│  ║
║      │      │                        EFFICIENT TOKEN BUFFER                                   ││  ║
║      │      │  ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐     ││  ║
║      │      │  │ t₀  │ t₁  │ t₂  │ t₃  │ t₄  │ t₅  │ t₆  │ t₇  │ t₈  │ t₉  │ ... │ tₙ  │     ││  ║
║      │      │  └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘     ││  ║
║      │      │  Single token-frame per step (uint16) • Reconstructs 4-frame history on sample ││  ║
║      │      └──────────────────────────────┬──────────────────────────────────────────────────┘│  ║
║      │                                     │                                                   │  ║
║      │                  ┌──────────────────┴──────────────────┐                                │  ║
║      │                  │                                     │                                │  ║
║      │                  ▼                                     ▼                                │  ║
║      │    ┌──────────────────────────────┐    ┌───────────────────────────────────────────┐   │  ║
║      │    │                              │    │                                           │   │  ║
║      │    │   TRANSFORMER WORLD MODEL    │    │              DQN AGENT                    │   │  ║
║      │    │                              │    │                                           │   │  ║
║      │    │  ┌────────────────────────┐  │    │  ┌─────────────────────────────────────┐  │   │  ║
║      │    │  │  Input: 4-frame        │  │    │  │  Input: 4-frame token history       │  │   │  ║
║      │    │  │  token history + action│  │    │  │  (4 × 336 = 1344 tokens)            │  │   │  ║
║      │    │  └───────────┬────────────┘  │    │  └──────────────────┬──────────────────┘  │   │  ║
║      │    │              │               │    │                     │                     │   │  ║
║      │    │              ▼               │    │                     ▼                     │   │  ║
║      │    │  ┌────────────────────────┐  │    │  ┌─────────────────────────────────────┐  │   │  ║
║      │    │  │  Token + Position +    │  │    │  │  Token Embeddings (d=256)           │  │   │  ║
║      │    │  │  Action Embeddings     │  │    │  │  + Learned Position Encoding        │  │   │  ║
║      │    │  └───────────┬────────────┘  │    │  └──────────────────┬──────────────────┘  │   │  ║
║      │    │              │               │    │                     │                     │   │  ║
║      │    │              ▼               │    │                     ▼                     │   │  ║
║      │    │  ┌────────────────────────┐  │    │  ┌─────────────────────────────────────┐  │   │  ║
║      │    │  │  Transformer Encoder   │  │    │  │  Transformer Encoder                │  │   │  ║
║      │    │  │  4 layers, 256 dim     │  │    │  │  4 layers, 256 dim, 4 heads         │  │   │  ║
║      │    │  │  4 attention heads     │  │    │  └──────────────────┬──────────────────┘  │   │  ║
║      │    │  └───────────┬────────────┘  │    │                     │                     │   │  ║
║      │    │              │               │    │                     ▼                     │   │  ║
║      │    │     ┌────────┼────────┐      │    │  ┌─────────────────────────────────────┐  │   │  ║
║      │    │     ▼        ▼        ▼      │    │  │  Dueling DQN Head                   │  │   │  ║
║      │    │  ┌──────┬────────┬───────┐   │    │  │  ┌─────────────┐  ┌──────────────┐  │  │   │  ║
║      │    │  │ Next │ Reward │ Done  │   │    │  │  │ Value V(s)  │  │ Advantage    │  │  │   │  ║
║      │    │  │Token │ Pred   │ Pred  │   │    │  │  │ Stream      │  │ A(s,a)       │  │  │   │  ║
║      │    │  │Logits│ Head   │ Head  │   │    │  │  └──────┬──────┘  └──────┬───────┘  │  │   │  ║
║      │    │  │(512) │        │       │   │    │  │         │                │          │  │   │  ║
║      │    │  └──────┴────────┴───────┘   │    │  │         └───────┬────────┘          │  │   │  ║
║      │    │                              │    │  │                 ▼                   │  │   │  ║
║      │    │  Learns to predict future    │    │  │  ┌─────────────────────────────┐    │  │   │  ║
║      │    │  observations, rewards, and  │    │  │  │ Q(s,a) = V(s) + A(s,a) -    │    │  │   │  ║
║      │    │  episode terminations        │    │  │  │          mean(A(s,·))       │    │  │   │  ║
║      │    │                              │    │  │  └──────────────┬──────────────┘    │  │   │  ║
║      │    └──────────────────────────────┘    │  │                 │                   │  │   │  ║
║      │                                        │  └─────────────────┼───────────────────┘  │   │  ║
║      │                                        │                    │                      │   │  ║
║      │                                        │                    ▼                      │   │  ║
║      │                                        │  ┌─────────────────────────────────────┐  │   │  ║
║      │                                        │  │  ε-Greedy Action Selection          │  │   │  ║
║      │                                        │  │  • Random action with prob ε        │  │   │  ║
║      │                                        │  │  • argmax Q(s,a) with prob 1-ε      │  │   │  ║
║      │                                        │  │  • ε decays: 1.0 → 0.01 over 1M     │  │───┼───┘
║      │                                        │  └─────────────────────────────────────┘  │   
║      │                                        │                                           │   
║      │                                        │  Training: Double DQN + PER               │   
║      │                                        │  • Target network soft updates (τ=0.001)  │   
║      │                                        │  • Prioritized Experience Replay          │   
║      │                                        │  • Gradient clipping (max_norm=10)        │   
║      │                                        │                                           │   
║      │                                        └───────────────────────────────────────────┘   
║      │                                                                                        
║      │                                              A G E N T                                 
║      │                                                                                        
║      └────────────────────────────────────────────────────────────────────────────────────────┘  
║                                                                                                    ║
╠═══════════════════════════════════════════════════════════════════════════════════════════════════╣
║                                      CONTINUOUS TRAINING LOOP                                      ║
╠═══════════════════════════════════════════════════════════════════════════════════════════════════╣
║                                                                                                    ║
║         ┌─────────────────────────────────────────────────────────────────────────────────┐       ║
║         │                                                                                 │       ║
║         │    ┌───────────┐      ┌───────────┐      ┌───────────┐      ┌───────────┐      │       ║
║         │    │           │      │           │      │           │      │           │      │       ║
║    ┌────┴───►│  COLLECT  │─────►│  TRAIN    │─────►│  TRAIN    │─────►│  EVALUATE │──────┴────┐  ║
║    │         │  STEPS    │      │  DQN      │      │  WORLD    │      │  AGENT    │           │  ║
║    │         │           │      │  AGENT    │      │  MODEL    │      │           │           │  ║
║    │         └───────────┘      └───────────┘      └───────────┘      └───────────┘           │  ║
║    │                                                                                          │  ║
║    │              │                   │                   │                  │                │  ║
║    │              ▼                   ▼                   ▼                  ▼                │  ║
║    │         ┌─────────┐        ┌─────────┐        ┌─────────┐        ┌─────────┐            │  ║
║    │         │ Agent   │        │ Sample  │        │ Sample  │        │ Run 10  │            │  ║
║    │         │ acts in │        │ batch   │        │ history │        │ greedy  │            │  ║
║    │         │ env,    │        │ from    │        │ seqs,   │        │ episodes│            │  ║
║    │         │ encode  │        │ buffer, │        │ predict │        │ report  │            │  ║
║    │         │ frames, │        │ compute │        │ next    │        │ mean    │            │  ║
║    │         │ store   │        │ TD loss │        │ tokens  │        │ reward  │            │  ║
║    │         │ tokens  │        │         │        │         │        │         │            │  ║
║    │         └─────────┘        └─────────┘        └─────────┘        └─────────┘            │  ║
║    │                                                                                          │  ║
║    └──────────────────────────────────────────────────────────────────────────────────────────┘  ║
║                                          Repeat until done                                        ║
║                                                                                                    ║
╚═══════════════════════════════════════════════════════════════════════════════════════════════════╝
```

## Usage

```powershell
python world_model/v3/train/train_continuous.py `
    --game mspacman `
    --vqvae checkpoints/vqvae.pt `
    --wm checkpoints/world_model.pt `
    --steps 1000000
```

## Research Foundation

| Paper | Contribution |
|-------|--------------|
| [DCWM](https://arxiv.org/abs/2503.00653) | Discrete codebook world models |
| [IRIS](https://arxiv.org/abs/2405.15914) | Transformer + discrete tokens |
| [DreamerV3](https://arxiv.org/abs/2301.04104) | Continuous training design |
| [CUCL](https://arxiv.org/abs/2311.14911) | Codebook rehearsal |

---

*Built for the ultimate world model. 🎮🧠*
